############ HW 2 MACHINE LEARNING ###############
library(tidyverse)
library(ggplot2)
library(UsingR)
library(car)
library(stats)
library(DescTools)
library(AppliedPredictiveModeling)
library(lmtest)
library(MASS)
library(glmnet)
library(vcdExtra)
library(gmodels)
library(InformationValue)
library(naniar)
library(dplyr)
library(caret)
library(leaps)
library(earth)
library(mgcv)
library(ROCR)
library(randomForest)
library(mlbench)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(pROC)
#### read in data #####
train <- read.csv("C:/Users/kat4538/Documents/MSA/FALL 3/machine learning/hw 2/insurance_t.csv")
set.seed(444)
rf <- randomForest(INS ~ ., data = train, ntree = 200,
mtry = 8, importance = TRUE)
############ HW 2 MACHINE LEARNING ###############
library(tidyverse)
library(ggplot2)
library(UsingR)
library(car)
library(stats)
library(DescTools)
library(AppliedPredictiveModeling)
library(lmtest)
library(MASS)
library(glmnet)
library(vcdExtra)
library(gmodels)
library(InformationValue)
library(naniar)
library(dplyr)
library(caret)
library(leaps)
library(earth)
library(mgcv)
library(ROCR)
library(randomForest)
library(mlbench)
library(xgboost)
library(Ckmeans.1d.dp)
library(pdp)
library(pROC)
#### read in data #####
train <- read.csv("https://github.com/pjokeefe10/Fall-3-HW-Team-Blue-15/raw/main/insurance_t.csv")
# DETERMINE TYPE OF VARIABLES
str(train)
# var <- sapply(train, n_distinct)
##### check which variables have missing #####
gg_miss_var(train) # 14 variables
na_count <-sapply(train, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
##### missing value imputation #####
#convert all categorical var to factors
col_names <- names(train)[c(2,7:8,12,14,18,20,22,24,26:27,29:30,36,38)]
train[,col_names] <- lapply(train[,col_names] , factor)
# create binary flag col for all variables
flag  = train %>%
mutate(across(everything(), ~ is.na(.x),
.names = 'FLAG_NA_{.col}'))
# drop col if there are no missing values
flag_sub = flag[39:ncol(flag)][colSums(abs(flag[39:ncol(flag)]), na.rm = TRUE) > 0]
# drop all cols that are not numerical (because we only want the flag for numerical variables)
drop_flag = c('FLAG_NA_DDA','FLAG_NA_DIRDEP','FLAG_NA_NSF','FLAG_NA_SAV'
,'FLAG_NA_ATM','FLAG_NA_CD','FLAG_NA_IRA','FLAG_NA_INV','FLAG_NA_MM','FLAG_NA_MMCRED',
'FLAG_NA_CC','FLAG_NA_CCPURC','FLAG_NA_SDB','FLAG_NA_INAREA','FLAG_NA_INS', 'FLAG_NA_BRANCH')
drop_vars <- names(flag_sub) %in% drop_flag
flag_sub_sub <- flag_sub[!drop_vars]
# add flags back to original data frame
train <- cbind(train, flag_sub_sub)
# mode for categorical var
calc_mode <- function(x){
# list the distinct / unique values
distinct_values <- unique(x)
# count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
train$INV <- if_else(is.na(train$INV), calc_mode(train$INV), train$INV)
train$CC <- if_else(is.na(train$CC), calc_mode(train$CC), train$CC)
train$CCPURC <- if_else(is.na(train$CCPURC), calc_mode(train$CCPURC), train$CCPURC)
# median for continuous var
train <- train %>%
mutate_if(is.numeric, function(x) ifelse(is.na(x), median(x, na.rm = T), x))
##### check each variable for separation problems #####
# create list of categorical variable names
all <- names(train)[c(2,7:8,12,14,18,20,22,24,26:27,29:30,36,38)]
# print table for each column against INS
for(i in all){
table_temp <- train %>%
dplyr::select(INS, i) %>%
table()
print(table_temp) #MMCRED has quasi-separation
}
# roll up categories
train$MMCRED <- as.character(train$MMCRED)
train$MMCRED[which(train$MMCRED > 2)] <- "3+" # new category for 3+ money market credits
table(train$INS, train$MMCRED)
set.seed(444)
rf <- randomForest(INS ~ ., data = train, ntree = 200,
mtry = 8, importance = TRUE)
# FINAL MODEL #
set.seed(444)
rf <- randomForest(INS ~ . - LORES - FLAG_NA_CRSCORE - FLAG_NA_INCOME
- FLAG_NA_ACCTAGE - SDB - NSFAMT - INAREA,
data = train, ntree = 200, mtry = 8, importance = TRUE)
train$INS <- as.factor(train$INS)
set.seed(444)
rf <- randomForest(INS ~ . - LORES - FLAG_NA_CRSCORE - FLAG_NA_INCOME
- FLAG_NA_ACCTAGE - SDB - NSFAMT - INAREA,
data = train, ntree = 200, mtry = 8, importance = TRUE)
train_p <- train
train_p$p_hat <- predict(rf, type = "prob")[,2]
p1 <- train_p$p_hat[train_p$INS == 1]
p0 <- train_p$p_hat[train_p$INS == 0]
#ROC curve
pred.rf <- prediction(train_p$p_hat, factor(train_p$INS))
perf.rf <- performance(pred.rf, measure = "tpr", x.measure = "fpr")
plot(perf.rf, lwd = 3, col = "dodgerblue3", main = paste0("Random Forest ROC Plot (AUC = ", round(AUROC(train_p$INS, train_p$p_hat), 3),")"),
xlab = "False Positive",
ylab = "True Positive")
abline(a = 0, b = 1, lty = 3)
# coefficient of discrimination
coef_discrim <- mean(p1) - mean(p0)
ggplot(train_p, aes(p_hat, fill = factor(INS))) +
geom_density(alpha = 0.7) +
labs(x = "Predicted Probability",
y = "Density",
fill = "Outcome",
title = "Discrimination Slope for Random Forest",
subtitle = paste("Coefficient of Discrimination = ",
round(coef_discrim, 3), sep = "")) +
scale_fill_manual(values = c("#1C86EE", "#FFB52E"),name = "Customer Decision", labels = c("Not Bought", "Bought")) +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5) )
##### XGBOOST #####
# Prepare data for XGBoost function - similar to what we did for glmnet
train_x <- model.matrix(INS ~ ., data = train)[, -1]
train_y <- as.numeric(train$INS)-1
set.seed(444)
# FINAL MODEL #
xgb <- xgboost(data = train_x, label = train_y,
subsample = 0.90, nrounds = 11, eta = 0.50,
max_depth = 4, objective = "binary:logistic",
eval_metric = "auc")
# ROC curve
train_p2 <- train
train_p2$y_pred <- predict(xgb, train_x)
p1xgb <- train_p2$y_pred[train_p2$INS == 1]
p0xgb <- train_p2$y_pred[train_p2$INS == 0]
#ROC curve
pred.xgb <- prediction(y_pred, factor(train_p$INS))
train_p2 <- train
train_p2$y_pred <- predict(xgb, train_x)
p1xgb <- train_p2$y_pred[train_p2$INS == 1]
p0xgb <- train_p2$y_pred[train_p2$INS == 0]
#ROC curve
pred.xgb <- prediction(train_p2$y_pred, factor(train_p$INS))
perf.xgb <- performance(pred.xgb, measure = "tpr", x.measure = "fpr")
plot(perf.xgb, lwd = 3, col = "dodgerblue3", main = paste0("XGBoost ROC Plot (AUC = ", round(AUROC(train_p2$INS, y_pred), 3),")"),
xlab = "False Positive",
ylab = "True Positive")
train_p2 <- train
train_p2$y_pred <- predict(xgb, train_x)
p1xgb <- train_p2$y_pred[train_p2$INS == 1]
p0xgb <- train_p2$y_pred[train_p2$INS == 0]
#ROC curve
pred.xgb <- prediction(train_p2$y_pred, factor(train_p$INS))
perf.xgb <- performance(pred.xgb, measure = "tpr", x.measure = "fpr")
plot(perf.xgb, lwd = 3, col = "dodgerblue3", main = paste0("XGBoost ROC Plot (AUC = ", round(AUROC(train_p2$INS, y_pred), 3),")"),
xlab = "False Positive",
ylab = "True Positive")
# ROC curve
train_p2 <- train
train_p2$y_pred <- predict(xgb, train_x)
p1xgb <- train_p2$y_pred[train_p2$INS == 1]
p0xgb <- train_p2$y_pred[train_p2$INS == 0]
#ROC curve
pred.xgb <- prediction(train_p2$y_pred, factor(train_p$INS))
perf.xgb <- performance(pred.xgb, measure = "tpr", x.measure = "fpr")
plot(perf.xgb, lwd = 3, col = "dodgerblue3", main = paste0("XGBoost ROC Plot (AUC = ", round(AUROC(train_p2$INS, train_p2$y_pred), 3),")"),
xlab = "False Positive",
ylab = "True Positive")
abline(a = 0, b = 1, lty = 3)
# coefficient of discrimination
coef_discrim <- mean(p1xgb) - mean(p0xgb)
ggplot(train_p2, aes(train_p2$y_pred, fill = factor(INS))) +
geom_density(alpha = 0.7) +
labs(x = "Predicted Probability",
y = "Density",
fill = "Outcome",
title = "Discrimination Slope for XGBoost",
subtitle = paste("Coefficient of Discrimination = ",
round(coef_discrim, 3), sep = "")) +
scale_fill_manual(values = c("#1C86EE", "#FFB52E"),name = "Customer Decision", labels = c("Not Bought", "Bought")) +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5) )
coef_discrim <- mean(p1xgb) - mean(p0xgb)
ggplot(train_p2, aes(y_pred, fill = factor(INS))) +
geom_density(alpha = 0.7) +
labs(x = "Predicted Probability",
y = "Density",
fill = "Outcome",
title = "Discrimination Slope for XGBoost",
subtitle = paste("Coefficient of Discrimination = ",
round(coef_discrim, 3), sep = "")) +
scale_fill_manual(values = c("#1C86EE", "#FFB52E"),name = "Customer Decision", labels = c("Not Bought", "Bought")) +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle =element_text(hjust = 0.5) )
View(train)
xgb.importance(feature_names = colnames(train_x), model = xgb)
View(train_x)
# Packages
library(tidyverse)
library(survival)
library(foreign)
library(ggplot2)
library(survminer)
library(rms)
library(flexsurv)
library(dplyr)
library(ciTools)
library(here)
library(visreg)
library(cmprsk)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(lubridate)
library(gmodels)
library(data.table)
counting_fin <- read_csv("https://github.com/pjokeefe10/Fall-3-HW-Team-Blue-15/blob/meghana/counting_fin.csv?raw=true")
full.model <- coxph(Surv( tstart, Hour, motor ) ~  factor(backup) + age +
factor(bridgecrane) + factor(servo) + factor(gear)  +
factor(trashrack) + slope + factor(elevation) +
factor(Time_at12) + factor(motor_on), data = counting_fin)
empty.model <- coxph(Surv( tstart, Hour, motor ) ~ factor(Time_at12), data = counting_fin)
alpha.f=0.03
for.model <- step(empty.model,
scope = list(lower=formula(empty.model),
upper=formula(full.model)),
direction = "backward", k = qchisq(alpha.f, 1, lower.tail = FALSE))
cox_2 <- coxph( Surv( tstart, Hour, motor ) ~  age +
slope + factor(Time_at12), data = counting_fin)
summary(cox_2)
alpha.f=0.03
for.model <- step(full.model,
scope = list(lower=formula(empty.model),
upper=formula(full.model)),
direction = "backward", k = qchisq(alpha.f, 1, lower.tail = FALSE))
summary(cox_1)
cox_1 <- coxph( Surv( tstart, Hour, motor ) ~  factor(backup) + age +
factor(bridgecrane) + factor(servo) + factor(gear)  +
factor(trashrack) + slope + factor(elevation) +
factor(Time_at12) + factor(motor_on), data = counting_fin)
summary(cox_1)
full.model <- coxph(Surv( tstart, Hour, motor ) ~  factor(backup) + age +
factor(bridgecrane) + factor(servo) + factor(gear) + slope + factor(elevation) +
factor(Time_at12) + factor(motor_on), data = counting_fin)
empty.model <- coxph(Surv( tstart, Hour, motor ) ~ factor(Time_at12), data = counting_fin)
alpha.f=0.03
for.model <- step(full.model,
scope = list(lower=formula(empty.model),
upper=formula(full.model)),
direction = "backward", k = qchisq(alpha.f, 1, lower.tail = FALSE))
View(counting_fin)
